<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>{{ site.name }}</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="{{ site.name }}" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Òscar Lorente
              </h1>
              <p>I am a Master's student in Computer Vision at Universitat Autònoma de Barcelona...
              </p>
              <p>
                ...
              </p>
              <p>
                ...
              </p>
              <p style="text-align:center">
                <a href="mailto:oscar.lorente.co@gmail.com"> Email</a> &nbsp;/&nbsp;
                <a href="pdfs/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/oscarlorente">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/lorenteoscar"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?view_op=list_works&hl=ca&user=QEaNKcQAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="https://avatars.githubusercontent.com/u/42140425?v=4">
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in computer vision and machine learning, especially in 3D vision and gesture recognition.
              </p>
            </td>
          </tr>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/video_surveillance.png" alt="video_surveillance_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Video Surveillance for Road Traffic Monitoring</strong></papertitle>
                    </a>
                    <br>
                    Pol Albacar, 
                    <strong>Òscar Lorente</strong>,
                    <a href="https://eddiemg.github.io/">Eduard Mainou</a>, 
                    Ian Riera
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04908">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Video-Surveillance-for-Road-Traffic-Monitoring">code</a>
                    <p>
                      Solution to the third track of the <a href="https://www.aicitychallenge.org/">AI-City Challenge</a>, that aims to track vehicles across multiple cameras placed in multiple intersections spread out over a city. The methodology followed focuses first in solving multi-tracking in a single camera and then extending it to multiple cameras using siamese networks and metric learning.
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/scene_understanding.png" alt="scene_understanding_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Scene Understanding for Autonomous Driving</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    Ian Riera, 
                    <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04905">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Scene-Understanding-for-Autonomous-Driving">code</a>
                    <p>
                      Study of the behaviour of different configurations of RetinaNet, Faster R-CNN and Mask R-CNN (Detectron2) by a qualitative and quantitative evaluation on KITTI-MOTS, MOTSChallenge and out of context datasets.
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/image_classification.png" alt="image_classification_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Image Classification with Classic and Deep Learning Techniques</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    Ian Riera, 
                    <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04895">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Image-Classification">code</a>
                    <p>
                      Image classifier using both classic computer vision techniques (Bag of Visual Words classifier using SVM) and deep learning techniques (MLPs, InceptionV3 and our own CNN, TinyNet).
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="{{site.baseurl}}/images/image_restoration_editing.png" alt="image_restoration_editing_png"
                style="width:auto; height:auto; max-width:100%;" >
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
                <papertitle><strong>Image Restoration and Segmentation with Optimization Techniques</strong></papertitle>
              </a>
              <br>
              <strong>Òscar Lorente</strong>, 
              <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>,
              Antoni Rodriguez
              <br>
              2020
              <br>
              <a href="https://github.com/oscarlorente/Image-Restoration-and-Segmentation">code</a>
              <p>
                Inpainting, Poisson editing, Chan-Vese segmentation, Markov Random Fields for image segmentation.
              </p>
            </td>
          </tr>
        </table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/museum_painting_retrieval.png" alt="museum_painting_retrieval_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Museum Painting Retrieval</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    Ian Riera, 
                    Shauryadeep Chaudhuri, 
                    Oriol Catalan, 
                    Víctor Casales 
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04891">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Museum-Painting-Retrieval">code</a>
                    <p>
                      Query by example CBIR system for finding paintings in a museum image collection using color, texture, text and feature descriptors in datasets with different perturbations in the images: noise, overlapping text boxes, color corruption and rotation.
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="{{site.baseurl}}/images/pedestrian-detection.png" alt="pedestrian_detection_png"
                style="width:auto; height:auto; max-width:100%;" >
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
                <papertitle><strong>Pedestrian Detection in 3D Point Clouds using Deep Neural Networks</strong></papertitle>
              </a>
              <br>
              <strong>Òscar Lorente</strong>, 
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>, 
              <a href="https://www.cd6.upc.edu/personnel.php?person=31">Santiago Royo</a>, 
              <a href="https://www.cd6.upc.edu/personnel.php?person=292">Ivan Caminal</a>
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2105.01151">arXiv</a> /
              <a href="{{site.baseurl}}/pdfs/pedestrian-detection.pdf">slides</a>
              <p>
                PointNet++ based architecture to classify pedestrians in LIDAR point clouds using 3D clusters obtained by projecting 2D labels.
              </p>
            </td>
          </tr>
        </table>

         

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

