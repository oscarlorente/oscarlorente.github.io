<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>{{ site.name }}</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="{{ site.name }}" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Òscar Lorente
              </h1>
              <p> I am a Research intern at <a href="https://www.iri.upc.edu/">IRI, (CSIC-UPC)</a>. I am interested in Computer Vision and Deep Learning, especially in 3D vision applications (AR/VR, 3D reconstruction...) and reinforcement learning. In my last work I combined parametric and non-parametric models to improve multi-view 3D human reconstruction in situations with very sparse views.
              </p>
              <p>
                I have a <a href="https://pagines.uab.cat/mcv/">M.Sc. degree in Computer Vision</a> from <a href="https://www.uab.cat/web/universitat-autonoma-de-barcelona-1345467954774.html">Universitat Autònoma de Barcelona</a>. I carried out my master's thesis at <a href="https://www.iri.upc.edu/">IRI (CSIC-UPC)</a>, advised by Dr. <a href="https://www.iri.upc.edu/people/fmoreno/index.html">Francesc Moreno-Noguer</a>, Dr. <a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giró-i-Nieto</a> (<a href="https://imatge.upc.edu/web/">GPI</a>) and <a href="https://www.iri.upc.edu/people/ecorona/">Enric Corona Puyané</a>. I completed my B.Sc. degree in <a href="https://telecos.upc.edu/en?set_language=en">Telecommunications Engineering</a> at <a href="https://www.upc.edu/en?set_language=en">Universitat Politècnica de Catalunya</a>, majoring in Audiovisual Systems, and I interned at the <a href="https://imatge.upc.edu/web/">Image Processing Group</a> and <a href="https://www.cd6.upc.edu/">CD6</a>, under the supervision of Dr. <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep Ramon Casas</a> and Dr. <a href="https://www.cd6.upc.edu/personnel.php?person=31">Santiago Royo Royo</a>, where I carried out my bachelor's thesis.
              </p>
              <p style="text-align:center">
                <a href="mailto:oscar.lorente.co@gmail.com"> Email</a> &nbsp;/&nbsp;
                <a href="pdfs/cv_new2.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/oscarlorente">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/lorenteoscar"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?view_op=list_works&hl=ca&user=QEaNKcQAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="{{site.baseurl}}/images/me.png">
            </td>
          </tr>
        </table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in computer vision and machine learning, especially in 3D vision and gesture recognition.
              </p>
            </td>
          </tr>
        </table> -->
        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Projects</h2>
            </td>
          </tr>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/video_surveillance.png" alt="video_surveillance_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Video Surveillance for Road Traffic Monitoring</strong></papertitle>
                    </a>
                    <br>
                    Pol Albacar, 
                    <strong>Òscar Lorente</strong>,
                    <a href="https://eddiemg.github.io/">Eduard Mainou</a>, 
                    <a href="https://ianriera.github.io/">Ian Riera</a>
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04908">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Video-Surveillance-for-Road-Traffic-Monitoring">code</a>
                    <p>
                      Solution to the third track of the <a href="https://www.aicitychallenge.org/">AI-City Challenge</a>, that aims to track vehicles across multiple cameras placed in multiple intersections spread out over a city. The methodology followed focuses first in solving multi-tracking in a single camera and then extending it to multiple cameras using siamese networks and metric learning.
                    </p>
                  </td>
                </tr>
              </table>
  
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/3D_reconstruction_urban_scenes.png" alt="3D_reconstruction_urban_scenes_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>3D Reconstruction of Urban Scenes</strong></papertitle>
                    </a>
                    <br>
                    Josep Brugués,
                    <strong>Òscar Lorente</strong>,
                    <a href="https://ianriera.github.io/">Ian Riera</a>,
                    Sergi García
                    <br>
                    2021
                    <br>
                    <a href="https://github.com/oscarlorente/3D-Reconstruction-of-Urban-Scenes">code</a> /
                    <a href="{{site.baseurl}}/pdfs/3D-reconstruction-urban-scenes.pdf">slides</a>
                    <p>
                      3D reconstruction of buildings from a set of images taken from different points of view (frontal images of the façades and aerial images). Rectify the perspective distortion from a single view, estimate essential and fundamental matrix, calibrate a camera with a planar pattern, estimate the depth of points in the scene given two images, generate new views of the scene, and compute a 3D model either from a set of calibrated or uncalibrated cameras (SfM).
                    </p>
                  </td>
                </tr>
              </table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/scene_understanding.png" alt="scene_understanding_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Scene Understanding for Autonomous Driving</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    <a href="https://ianriera.github.io/">Ian Riera</a>, 
                    <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04905">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Scene-Understanding-for-Autonomous-Driving">code</a>
                    <p>
                      Study of the behaviour of different configurations of RetinaNet, Faster R-CNN and Mask R-CNN (Detectron2) by a qualitative and quantitative evaluation on KITTI-MOTS, MOTSChallenge and out of context datasets.
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/image_classification.png" alt="image_classification_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Image Classification with Classic and Deep Learning Techniques</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    <a href="https://ianriera.github.io/">Ian Riera</a>, 
                    <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04895">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Image-Classification">code</a>
                    <p>
                      Image classifier using both classic computer vision techniques (Bag of Visual Words classifier using SVM) and deep learning techniques (MLPs, InceptionV3 and our own CNN: TinyNet).
                    </p>
                  </td>
                </tr>
              </table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
                    <img src="{{site.baseurl}}/images/museum_painting_retrieval.png" alt="museum_painting_retrieval_png"
                      style="width:auto; height:auto; max-width:100%;" >
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                      <papertitle><strong>Museum Painting Retrieval</strong></papertitle>
                    </a>
                    <br>
                    <strong>Òscar Lorente</strong>, 
                    <a href="https://ianriera.github.io/">Ian Riera</a>, 
                    Shauryadeep Chaudhuri, 
                    Oriol Catalan, 
                    Víctor Casales 
                    <br>
                    <em>arXiv</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2105.04891">arXiv</a> /
                    <a href="https://github.com/oscarlorente/Museum-Painting-Retrieval">code</a>
                    <p>
                      Query by example CBIR system for finding paintings in a museum image collection using color, texture, text and feature descriptors in datasets with different perturbations in the images: noise, overlapping text boxes, color corruption and rotation.
                    </p>
                  </td>
                </tr>
              </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
              <img src="{{site.baseurl}}/images/pedestrian-detection.png" alt="pedestrian_detection_png"
                style="width:auto; height:auto; max-width:100%;" >
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
                <papertitle><strong>Pedestrian Detection in 3D Point Clouds using Deep Neural Networks</strong></papertitle>
              </a>
              <br>
              <strong>Òscar Lorente</strong>, 
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>, 
              <a href="https://www.cd6.upc.edu/personnel.php?person=31">Santiago Royo</a>, 
              <a href="https://www.cd6.upc.edu/personnel.php?person=292">Ivan Caminal</a>
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2105.01151">arXiv</a> /
              <a href="{{site.baseurl}}/pdfs/pedestrian-detection.pdf">slides</a>
              <p>
                PointNet++ based architecture to classify pedestrians in LIDAR point clouds using 3D clusters obtained by projecting 2D labels.
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
              <img src="{{site.baseurl}}/images/image_restoration_editing.png" alt="image_restoration_editing_png"
                style="width:auto; height:auto; max-width:100%;" >
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
                <papertitle><strong>Image Restoration and Segmentation with Optimization Techniques</strong></papertitle>
              </a>
              <br>
              <strong>Òscar Lorente</strong>, 
              <a href="https://adityassrana.github.io/blog/about">Aditya Rana</a>,
              Antoni Rodriguez
              <br>
              2020
              <br>
              <a href="https://github.com/oscarlorente/Image-Restoration-and-Segmentation">code</a> /
              <a href="{{site.baseurl}}/pdfs/image-restoration.pdf">slides</a>
              <p>
                Implement different optimization techniques to solve specific tasks: inpainting, Poisson editing, Chan-Vese segmentation and Markov Random Fields for image segmentation.
              </p>
            </td>
          </tr>
        </table>

         

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

